# -*- coding: utf-8 -*-
"""Conflict points.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EW9xAsvEofxSa3VLppOgB-xx9-nU8Kot
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q supervision ultralytics
import cv2
import matplotlib.pyplot as plt
import numpy as np
import supervision as sv
print("supervision.__version__:", sv.__version__)

from tqdm import tqdm
from ultralytics import YOLO
from supervision.assets import VideoAssets, download_assets
from collections import defaultdict, deque

# Commented out IPython magic to ensure Python compatibility.
import os
HOME = os.getcwd()
print(HOME)

# %cd {HOME}

# %cd '/content/drive/My Drive/Colab Notebooks'

# Access the video file
SOURCE_VIDEO_PATH = '/content/drive/MyDrive/Colab Notebooks/chandi_5min_1.mp4'
TARGET_VIDEO_PATH = '/content/drive/MyDrive/Colab Notebooks/result.mp4'
CONFIDENCE_THRESHOLD = 0.5
IOU_THRESHOLD = 0.5
MODEL = '/content/drive/MyDrive/Colab Notebooks/bestJAT8x.pt'
MODEL_RESOLUTION = 1280

!pip install --upgrade google-auth-oauthlib google-auth-httplib2 google-auth

from google.colab import drive
drive.mount('/content/drive')

# Define custom vertices for your ROI (example coordinates, adjust as needed)
roi_vertices = np.array([[4, 1695],[1096, 1130],[1272, 940],[1430, 592],[1446, 308],[1422, 16],[2328, 12],[2492, 140],[2680, 192],[2928, 200],[3280, 80],[3480, 0],
                         [3580, 216],[3700, 155],[3840, 512],[3216, 844],[2368, 2028],[2324, 2144],[1408, 2152],[1196, 2060],[928, 2096],[756, 2152],[464, 1908],[104, 2076]])
# settings
LINE_START_1 = sv.Point(808, 1142)
LINE_END_1 = sv.Point(1172, 2070)

LINE_START_2 = sv.Point(3360, 654)
LINE_END_2 = sv.Point(3092, 142)

LINE_START_3 = sv.Point(2196, 10)
LINE_END_3 = sv.Point(1364, 182)

LINE_START_4 = sv.Point(1508, 2154)
LINE_END_4 = sv.Point(2364, 2074)

frame_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH)
frame_iterator = iter(frame_generator)
frame = next(frame_iterator)

annotated_frame = frame.copy()
annotated_frame = sv.draw_polygon(scene=annotated_frame, polygon=roi_vertices, color=sv.Color.RED, thickness=4)
sv.plot_image(annotated_frame)

model = YOLO(MODEL)
model.fuse()
# dict maping class_id to class_name
CLASS_NAMES_DICT = model.model.names
# class_ids of interest
CLASS_ID = [0, 1, 2, 3, 4, 5]

from typing import List
# converts Detections into format that can be consumed by match_detections_with_tracks function
def detections2boxes(detections: sv.Detections) -> np.ndarray:
    return np.hstack((
        detections.xyxy,
        detections.confidence[:, np.newaxis]
    ))
# converts List[ByteTracker] into format that can be consumed by match_detections_with_tracks function
def tracks2boxes(tracks: List[sv.ByteTrack]) -> np.ndarray:
    return np.array([
        track.tlbr
        for track
        in tracks
    ], dtype=float)
# matches our bounding boxes with predictions
def match_detections_with_tracks(
    detections: sv.Detections,
    tracks: List[sv.ByteTrack]
) -> sv.Detections:
    if not np.any(detections.xyxy) or len(tracks) == 0:
        return np.empty((0,))

    tracks_boxes = tracks2boxes(tracks=tracks)
    iou = sv.box_iou_batch(tracks_boxes, detections.xyxy)
    track2detection = np.argmax(iou, axis=1)

    tracker_ids = [None] * len(detections)

    for tracker_index, detection_index in enumerate(track2detection):
        if iou[tracker_index, detection_index] != 0:
            tracker_ids[detection_index] = tracks[tracker_index].track_id

    return tracker_ids

pip install --upgrade supervision

tracker = sv.ByteTrack()
box_annotator = sv.BoundingBoxAnnotator()
label_annotator = sv.LabelAnnotator()

def callback(frame: np.ndarray, _: int) -> np.ndarray:
    results = model(frame)[0]
    detections = sv.Detections.from_ultralytics(results)
    detections = tracker.update_with_detections(detections)

    labels = [
        f"#{tracker_id} {results.names[class_id]}"
        for class_id, tracker_id
        in zip(detections.class_id, detections.tracker_id)
    ]

    annotated_frame = box_annotator.annotate(
        frame.copy(), detections=detections)
    return label_annotator.annotate(
        annotated_frame, detections=detections, labels=labels)

sv.process_video(
    source_path=SOURCE_VIDEO_PATH,
    target_path=TARGET_VIDEO_PATH,
    callback=callback
)

pixels_to_meters = 0.054
moving_average_window = 5
# Create a dictionary to store speeds distances for each tracker_id
speeds = defaultdict(list)
accelerations = defaultdict(list)
# Initialize data structures to store MTTC and DRAC for each pair of vehicles
mttc_data = defaultdict(lambda: defaultdict(list))
drac_data = defaultdict(lambda: defaultdict(list))

video_info = sv.VideoInfo.from_video_path(video_path=SOURCE_VIDEO_PATH)
frame_generator = sv.get_video_frames_generator(source_path=SOURCE_VIDEO_PATH)

# create LineCounter instance
line_counter_1 = sv.LineZone(start=LINE_START_1, end=LINE_END_1)
line_counter_2 = sv.LineZone(start=LINE_START_2, end=LINE_END_2)
line_counter_3 = sv.LineZone(start=LINE_START_3, end=LINE_END_3)
line_counter_4 = sv.LineZone(start=LINE_START_4, end=LINE_END_4)

# import cv2
# import numpy as np
# from collections import defaultdict, deque
# from tqdm import tqdm

# # --- Tracker and annotator initialization ---
# byte_track = sv.ByteTrack(frame_rate=video_info.fps, track_activation_threshold=CONFIDENCE_THRESHOLD)
# thickness = 2
# text_scale = 1
# bounding_box_annotator = sv.BoxAnnotator(thickness=thickness)
# label_annotator = sv.LabelAnnotator(text_scale=1, text_thickness=2, text_position=sv.Position.TOP_CENTER)
# trace_annotator = sv.TraceAnnotator(thickness=thickness, trace_length=999999, position=sv.Position.CENTER)
# line_annotator = sv.LineZoneAnnotator(thickness=2, text_thickness=2, text_scale=1)
# polygon_zone = sv.PolygonZone(polygon=roi_vertices)
# coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))
# moving_average_window = 5
# pixels_to_meters = 0.054

# speeds = defaultdict(list)
# accelerations = defaultdict(list)
# mttc_data = defaultdict(lambda: defaultdict(list))
# drac_data = defaultdict(lambda: defaultdict(list))

# # --- NEW: Initialize trajectory dictionary ---
# trajectories = defaultdict(list)  # {tracker_id: [(frame_number, (x, y)), ...]}

# # --- Open target video for writing ---
# with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:
#     for frame_number, frame in enumerate(tqdm(frame_generator, total=video_info.total_frames)):
#         results = model(frame)
#         detections = sv.Detections(
#             xyxy=results[0].boxes.xyxy.cpu().numpy(),
#             confidence=results[0].boxes.conf.cpu().numpy(),
#             class_id=results[0].boxes.cls.cpu().numpy().astype(int)
#         )
#         mask = np.array([class_id in CLASS_ID for class_id in detections.class_id], dtype=bool)

#         # filter out detections by class and confidence
#         detections = detections[detections.confidence > CONFIDENCE_THRESHOLD]
#         detections = detections[detections.class_id != 0]
#         # filter out detections outside the zone
#         detections = detections[polygon_zone.trigger(detections)]
#         # refine detections using non-max suppression
#         detections = detections.with_nms(IOU_THRESHOLD)
#         # pass detection through the tracker
#         detections = byte_track.update_with_detections(detections=detections)
#         points = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)
#         line_counter_1.trigger(detections=detections)
#         line_counter_2.trigger(detections=detections)
#         line_counter_3.trigger(detections=detections)
#         line_counter_4.trigger(detections=detections)

#         # Count the total number of objects detected and tracked
#         total_objects_detected = len(detections)
#         class_counters = {class_id: np.sum(detections.class_id == class_id) for class_id in CLASS_ID}
#         count_text = f"\nTotal: {total_objects_detected}\n"
#         count_text += "\n".join([f"{CLASS_NAMES_DICT[class_id]}: {count}" for class_id, count in class_counters.items()])

#         # --- Store detections position and build trajectories ---
#         for tracker_id, (x, y) in zip(detections.tracker_id, points):
#             coordinates[tracker_id].append((x, y))
#             trajectories[tracker_id].append((frame_number, (x, y)))  # <-- This line collects (frame, (x, y))
#             if len(coordinates[tracker_id]) >= moving_average_window:
#                 smoothed_x = np.mean([pt[0] for pt in list(coordinates[tracker_id])[-moving_average_window:]])
#                 smoothed_y = np.mean([pt[1] for pt in list(coordinates[tracker_id])[-moving_average_window:]])
#                 coordinates[tracker_id][-1] = (smoothed_x, smoothed_y)

#         # format labels and calculate speed/acceleration
#         labels = []
#         for tracker_id, class_id, confidence in zip(detections.tracker_id, detections.class_id, detections.confidence):
#             class_name = CLASS_NAMES_DICT[class_id]
#             if len(coordinates[tracker_id]) < video_info.fps / 2:
#                 labels.append(f"#{tracker_id}-{class_name}-{confidence:.2f}")
#             else:
#                 coordinate_start = coordinates[tracker_id][0]
#                 coordinate_end = coordinates[tracker_id][-1]
#                 distance_pixels = np.sqrt((coordinate_end[0] - coordinate_start[0])**2 +
#                                           (coordinate_end[1] - coordinate_start[1])**2)
#                 distance = distance_pixels * pixels_to_meters
#                 time = len(coordinates[tracker_id]) / video_info.fps
#                 speed = (distance / time) * 3.6  # m/s to km/h
#                 acceleration = speed / time
#                 labels.append(f"#{tracker_id}-{class_name}-{confidence:.2f}-{int(speed)}km/h")
#                 speeds[tracker_id].append(speed)
#                 accelerations[tracker_id].append(acceleration)

#         # Calculate MTTC and DRAC for each pair of vehicles (optional, as before)
#         for tracker_id_1, class_id_1, speed_1, acceleration_1, coordinates_1 in zip(
#             detections.tracker_id, detections.class_id, accelerations, speeds, points
#         ):
#             for tracker_id_2, class_id_2, speed_2, acceleration_2, coordinates_2 in zip(
#                 detections.tracker_id, detections.class_id, speeds, accelerations, points
#             ):
#                 if tracker_id_1 != tracker_id_2:
#                     relative_distance = abs(coordinates_1[1] - coordinates_2[1])
#                     relative_speed = abs(speed_2 - speed_1)
#                     relative_acceleration = abs(acceleration_2 - acceleration_1)
#                     discriminant = relative_speed**2 + 2 * relative_acceleration * (relative_distance)
#                     if discriminant < 0:
#                         mttc = float('inf')
#                     else:
#                         root1 = (-relative_speed + np.sqrt(discriminant)) / relative_acceleration if relative_acceleration != 0 else float('inf')
#                         root2 = (-relative_speed - np.sqrt(discriminant)) / relative_acceleration if relative_acceleration != 0 else float('inf')
#                         if root1 > 0 and root2 > 0:
#                             mttc = min(root1, root2)
#                         elif root1 > 0:
#                             mttc = root1
#                         elif root2 > 0:
#                             mttc = root2
#                         else:
#                             mttc = float('inf')
#                     mttc_data[tracker_id_1][tracker_id_2].append(mttc)
#                     drac = (speed_2 ** 2 - speed_1 ** 2) / (2 * relative_distance) if relative_distance > 0 else float('inf')
#                     drac_data[tracker_id_1][tracker_id_2].append(drac)

#         # annotate frame
#         annotated_frame = frame.copy()
#         annotated_frame = trace_annotator.annotate(scene=annotated_frame, detections=detections)
#         annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections)
#         annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)
#         annotated_frame = line_annotator.annotate(frame=annotated_frame, line_counter=line_counter_1)
#         annotated_frame = line_annotator.annotate(frame=annotated_frame, line_counter=line_counter_2)
#         annotated_frame = line_annotator.annotate(frame=annotated_frame, line_counter=line_counter_3)
#         annotated_frame = line_annotator.annotate(frame=annotated_frame, line_counter=line_counter_4)

#         # Add the count text to the frame
#         for i, line in enumerate(count_text.split('\n')):
#             cv2.putText(annotated_frame, line, (10, 10 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

#         # add frame to target video
#         sink.write_frame(annotated_frame)

# import cv2
# import numpy as np
# import pickle
# import os
# from collections import defaultdict, deque
# from tqdm import tqdm
# import supervision as sv  # Make sure supervision is installed and imported

# # --- Tracker and annotator initialization ---
# byte_track = sv.ByteTrack(frame_rate=video_info.fps, track_activation_threshold=CONFIDENCE_THRESHOLD)
# thickness = 2
# text_scale = 1
# bounding_box_annotator = sv.BoxAnnotator(thickness=thickness)
# label_annotator = sv.LabelAnnotator(text_scale=1, text_thickness=2, text_position=sv.Position.TOP_CENTER)
# trace_annotator = sv.TraceAnnotator(thickness=thickness, trace_length=999999, position=sv.Position.CENTER)
# line_annotator = sv.LineZoneAnnotator(thickness=2, text_thickness=2, text_scale=1)
# polygon_zone = sv.PolygonZone(polygon=roi_vertices)
# coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))
# moving_average_window = 5
# pixels_to_meters = 0.054

# speeds = defaultdict(list)
# accelerations = defaultdict(list)
# mttc_data = defaultdict(lambda: defaultdict(list))
# drac_data = defaultdict(lambda: defaultdict(list))
# trajectories = defaultdict(list)
# conflict_points = set()

# # --- Open target video for writing ---
# with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:
#     for frame_number, frame in enumerate(tqdm(frame_generator, total=video_info.total_frames)):
#         results = model(frame)
#         detections = sv.Detections(
#             xyxy=results[0].boxes.xyxy.cpu().numpy(),
#             confidence=results[0].boxes.conf.cpu().numpy(),
#             class_id=results[0].boxes.cls.cpu().numpy().astype(int)
#         )
#         detections = detections[detections.confidence > CONFIDENCE_THRESHOLD]
#         detections = detections[detections.class_id != 0]
#         detections = detections[polygon_zone.trigger(detections)]
#         detections = detections.with_nms(IOU_THRESHOLD)
#         detections = byte_track.update_with_detections(detections=detections)

#         points = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)
#         line_counter_1.trigger(detections=detections)
#         line_counter_2.trigger(detections=detections)
#         line_counter_3.trigger(detections=detections)
#         line_counter_4.trigger(detections=detections)

#         total_objects_detected = len(detections)
#         class_counters = {class_id: np.sum(detections.class_id == class_id) for class_id in CLASS_ID}
#         count_text = f"\nTotal: {total_objects_detected}\n" + "\n".join([f"{CLASS_NAMES_DICT[class_id]}: {count}" for class_id, count in class_counters.items()])

#         for tracker_id, (x, y) in zip(detections.tracker_id, points):
#             coordinates[tracker_id].append((x, y))
#             trajectories[tracker_id].append((frame_number, (x, y)))
#             if len(coordinates[tracker_id]) >= moving_average_window:
#                 smoothed_x = np.mean([pt[0] for pt in list(coordinates[tracker_id])[-moving_average_window:]])
#                 smoothed_y = np.mean([pt[1] for pt in list(coordinates[tracker_id])[-moving_average_window:]])
#                 coordinates[tracker_id][-1] = (smoothed_x, smoothed_y)

#         labels = []
#         for tracker_id, class_id, confidence in zip(detections.tracker_id, detections.class_id, detections.confidence):
#             class_name = CLASS_NAMES_DICT[class_id]
#             if len(coordinates[tracker_id]) < video_info.fps / 2:
#                 labels.append(f"#{tracker_id}-{class_name}-{confidence:.2f}")
#             else:
#                 coordinate_start = coordinates[tracker_id][0]
#                 coordinate_end = coordinates[tracker_id][-1]
#                 distance_pixels = np.linalg.norm(np.subtract(coordinate_end, coordinate_start))
#                 distance = distance_pixels * pixels_to_meters
#                 time = len(coordinates[tracker_id]) / video_info.fps
#                 speed = (distance / time) * 3.6
#                 acceleration = speed / time
#                 labels.append(f"#{tracker_id}-{class_name}-{confidence:.2f}-{int(speed)}km/h")
#                 speeds[tracker_id].append(speed)
#                 accelerations[tracker_id].append(acceleration)

#         # --- Conflict detection ---
#         for i, tracker_id_1 in enumerate(detections.tracker_id):
#             for j, tracker_id_2 in enumerate(detections.tracker_id):
#                 if i >= j or tracker_id_1 == tracker_id_2:
#                     continue
#                 coord1 = points[i]
#                 coord2 = points[j]
#                 if len(speeds[tracker_id_1]) == 0 or len(speeds[tracker_id_2]) == 0 or len(accelerations[tracker_id_1]) == 0 or len(accelerations[tracker_id_2]) == 0:
#                     continue
#                 speed_1 = speeds[tracker_id_1][-1]
#                 speed_2 = speeds[tracker_id_2][-1]
#                 acc_1 = accelerations[tracker_id_1][-1]
#                 acc_2 = accelerations[tracker_id_2][-1]
#                 relative_distance = abs(coord1[1] - coord2[1])
#                 relative_speed = abs(speed_2 - speed_1)
#                 relative_acc = abs(acc_2 - acc_1)
#                 discriminant = relative_speed**2 + 2 * relative_acc * relative_distance
#                 if discriminant < 0:
#                     mttc = float('inf')
#                 else:
#                     root1 = (-relative_speed + np.sqrt(discriminant)) / relative_acc if relative_acc != 0 else float('inf')
#                     root2 = (-relative_speed - np.sqrt(discriminant)) / relative_acc if relative_acc != 0 else float('inf')
#                     mttc = min([r for r in [root1, root2] if r > 0], default=float('inf'))

#                 drac = (speed_2**2 - speed_1**2) / (2 * relative_distance) if relative_distance > 0 else float('inf')
#                 mttc_data[tracker_id_1][tracker_id_2].append(mttc)
#                 drac_data[tracker_id_1][tracker_id_2].append(drac)

#                 if mttc < 3 and drac > 1:  # You can tune these thresholds
#                     conflict_points.add((tracker_id_1, tracker_id_2))

#         # --- Annotation ---
#         annotated_frame = frame.copy()
#         annotated_frame = trace_annotator.annotate(scene=annotated_frame, detections=detections)
#         annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections)
#         annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)
#         annotated_frame = line_annotator.annotate(frame=annotated_frame, line_counter=line_counter_1)
#         annotated_frame = line_annotator.annotate(frame=annotated_frame, line_counter=line_counter_2)
#         annotated_frame = line_annotator.annotate(frame=annotated_frame, line_counter=line_counter_3)
#         annotated_frame = line_annotator.annotate(frame=annotated_frame, line_counter=line_counter_4)
#         for i, line in enumerate(count_text.split('\n')):
#             cv2.putText(annotated_frame, line, (10, 10 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

#         sink.write_frame(annotated_frame)

# # --- Save data to Google Drive ---
# output_dir = "/content/drive/MyDrive/Colab Notebooks"
# os.makedirs(output_dir, exist_ok=True)

# with open(os.path.join(output_dir, "trajectories.pkl"), "wb") as f:
#     pickle.dump(dict(trajectories), f)

# with open(os.path.join(output_dir, "speeds.pkl"), "wb") as f:
#     pickle.dump(dict(speeds), f)

# with open(os.path.join(output_dir, "accelerations.pkl"), "wb") as f:
#     pickle.dump(dict(accelerations), f)

# with open(os.path.join(output_dir, "mttc_data.pkl"), "wb") as f:
#     pickle.dump({k: dict(v) for k, v in mttc_data.items()}, f)

# with open(os.path.join(output_dir, "drac_data.pkl"), "wb") as f:
#     pickle.dump({k: dict(v) for k, v in drac_data.items()}, f)

# with open(os.path.join(output_dir, "conflict_points.pkl"), "wb") as f:
#     pickle.dump(conflict_points, f)

# print("✅ All tracking and conflict data saved to Google Drive at:", output_dir)

import cv2
import numpy as np
import pickle
import os
from collections import defaultdict, deque
from tqdm import tqdm
import matplotlib.pyplot as plt
import supervision as sv  # Ensure supervision is installed
from PIL import Image

# --- Output Directory ---
output_dir = "/content/drive/MyDrive/Colab Notebooks/conflict_output"
frames_dir = os.path.join(output_dir, "frames")
os.makedirs(frames_dir, exist_ok=True)

# --- Tracker and annotator initialization ---
byte_track = sv.ByteTrack(frame_rate=video_info.fps, track_activation_threshold=CONFIDENCE_THRESHOLD)
bounding_box_annotator = sv.BoxAnnotator(thickness=2)
label_annotator = sv.LabelAnnotator(text_scale=1, text_thickness=2, text_position=sv.Position.TOP_CENTER)
trace_annotator = sv.TraceAnnotator(thickness=2, trace_length=999999, position=sv.Position.CENTER)
line_annotator = sv.LineZoneAnnotator(thickness=2, text_thickness=2, text_scale=1)
polygon_zone = sv.PolygonZone(polygon=roi_vertices)

# --- Variables ---
coordinates = defaultdict(lambda: deque(maxlen=video_info.fps))
speeds = defaultdict(list)
accelerations = defaultdict(list)
trajectories = defaultdict(list)
mttc_data = defaultdict(lambda: defaultdict(list))
drac_data = defaultdict(lambda: defaultdict(list))
conflict_points = set()
conflict_coords = []

moving_average_window = 5
pixels_to_meters = 0.054

# --- Open target video for writing ---
with sv.VideoSink(TARGET_VIDEO_PATH, video_info) as sink:
    for frame_number, frame in enumerate(tqdm(frame_generator, total=video_info.total_frames)):
        results = model(frame)
        detections = sv.Detections(
            xyxy=results[0].boxes.xyxy.cpu().numpy(),
            confidence=results[0].boxes.conf.cpu().numpy(),
            class_id=results[0].boxes.cls.cpu().numpy().astype(int)
        )
        detections = detections[detections.confidence > CONFIDENCE_THRESHOLD]
        detections = detections[detections.class_id != 0]
        detections = detections[polygon_zone.trigger(detections)]
        detections = detections.with_nms(IOU_THRESHOLD)
        detections = byte_track.update_with_detections(detections=detections)

        points = detections.get_anchors_coordinates(anchor=sv.Position.BOTTOM_CENTER)
        line_counter_1.trigger(detections=detections)
        line_counter_2.trigger(detections=detections)
        line_counter_3.trigger(detections=detections)
        line_counter_4.trigger(detections=detections)

        # --- Track and smooth coordinates ---
        for tracker_id, (x, y) in zip(detections.tracker_id, points):
            coordinates[tracker_id].append((x, y))
            trajectories[tracker_id].append((frame_number, (x, y)))
            if len(coordinates[tracker_id]) >= moving_average_window:
                smoothed_x = np.mean([pt[0] for pt in list(coordinates[tracker_id])[-moving_average_window:]])
                smoothed_y = np.mean([pt[1] for pt in list(coordinates[tracker_id])[-moving_average_window:]])
                coordinates[tracker_id][-1] = (smoothed_x, smoothed_y)

        # --- Labels and speeds ---
        labels = []
        for tracker_id, class_id, confidence in zip(detections.tracker_id, detections.class_id, detections.confidence):
            class_name = CLASS_NAMES_DICT[class_id]
            if len(coordinates[tracker_id]) < video_info.fps / 2:
                labels.append(f"#{tracker_id}-{class_name}-{confidence:.2f}")
            else:
                coord_start = coordinates[tracker_id][0]
                coord_end = coordinates[tracker_id][-1]
                distance = np.linalg.norm(np.subtract(coord_end, coord_start)) * pixels_to_meters
                time = len(coordinates[tracker_id]) / video_info.fps
                speed = (distance / time) * 3.6
                acceleration = speed / time
                labels.append(f"#{tracker_id}-{class_name}-{confidence:.2f}-{int(speed)}km/h")
                speeds[tracker_id].append(speed)
                accelerations[tracker_id].append(acceleration)

        # --- Conflict detection ---
        for i, tracker_id_1 in enumerate(detections.tracker_id):
            for j, tracker_id_2 in enumerate(detections.tracker_id):
                if i >= j or tracker_id_1 == tracker_id_2:
                    continue
                coord1 = points[i]
                coord2 = points[j]
                if not (speeds[tracker_id_1] and speeds[tracker_id_2] and accelerations[tracker_id_1] and accelerations[tracker_id_2]):
                    continue
                speed1, speed2 = speeds[tracker_id_1][-1], speeds[tracker_id_2][-1]
                acc1, acc2 = accelerations[tracker_id_1][-1], accelerations[tracker_id_2][-1]
                rel_dist = abs(coord1[1] - coord2[1])
                rel_speed = abs(speed2 - speed1)
                rel_acc = abs(acc2 - acc1)
                discriminant = rel_speed**2 + 2 * rel_acc * rel_dist
                mttc = float('inf')
                if discriminant >= 0:
                    roots = [
                        (-rel_speed + np.sqrt(discriminant)) / rel_acc if rel_acc != 0 else float('inf'),
                        (-rel_speed - np.sqrt(discriminant)) / rel_acc if rel_acc != 0 else float('inf')
                    ]
                    mttc = min([r for r in roots if r > 0], default=float('inf'))
                drac = (speed2**2 - speed1**2) / (2 * rel_dist) if rel_dist > 0 else float('inf')
                mttc_data[tracker_id_1][tracker_id_2].append(mttc)
                drac_data[tracker_id_1][tracker_id_2].append(drac)

                if mttc < 3 and drac > 1:
                    conflict_points.add((tracker_id_1, tracker_id_2))
                    conflict_coords.append(((coord1[0] + coord2[0]) / 2, (coord1[1] + coord2[1]) / 2))
                    conflict_frame_path = os.path.join(frames_dir, f"frame_{frame_number}.jpg")
                    cv2.imwrite(conflict_frame_path, frame)

        # --- Annotation ---
        annotated_frame = frame.copy()
        annotated_frame = trace_annotator.annotate(scene=annotated_frame, detections=detections)
        annotated_frame = bounding_box_annotator.annotate(scene=annotated_frame, detections=detections)
        annotated_frame = label_annotator.annotate(scene=annotated_frame, detections=detections, labels=labels)

        for counter in [line_counter_1, line_counter_2, line_counter_3, line_counter_4]:
            annotated_frame = line_annotator.annotate(frame=annotated_frame, line_counter=counter)

        # --- Prepare counter text ---
        try:
            count_text = (
                f"Line1: {line_counter_1.current_count}  "
                f"Line2: {line_counter_2.current_count}  "
                f"Line3: {line_counter_3.current_count}  "
                f"Line4: {line_counter_4.current_count}"
            )
        except:
            count_text = ""

        for i, line in enumerate(count_text.split('\n')):
            cv2.putText(annotated_frame, line, (10, 30 + i * 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

        sink.write_frame(annotated_frame)

# --- Save all data ---
with open(os.path.join(output_dir, "trajectories.pkl"), "wb") as f:
    pickle.dump(dict(trajectories), f)
with open(os.path.join(output_dir, "speeds.pkl"), "wb") as f:
    pickle.dump(dict(speeds), f)
with open(os.path.join(output_dir, "accelerations.pkl"), "wb") as f:
    pickle.dump(dict(accelerations), f)
with open(os.path.join(output_dir, "mttc_data.pkl"), "wb") as f:
    pickle.dump({k: dict(v) for k, v in mttc_data.items()}, f)
with open(os.path.join(output_dir, "drac_data.pkl"), "wb") as f:
    pickle.dump({k: dict(v) for k, v in drac_data.items()}, f)
with open(os.path.join(output_dir, "conflict_points.pkl"), "wb") as f:
    pickle.dump(conflict_points, f)
with open(os.path.join(output_dir, "conflict_coords.pkl"), "wb") as f:
    pickle.dump(conflict_coords, f)

# --- Save conflict plot ---
if conflict_coords:
    x_vals, y_vals = zip(*conflict_coords)
    plt.figure(figsize=(12, 8))
    plt.scatter(x_vals, y_vals, c='red', label="Conflict Points", s=20)
    plt.gca().invert_yaxis()
    plt.title("Detected Conflict Points")
    plt.xlabel("X (pixels)")
    plt.ylabel("Y (pixels)")
    plt.legend()
    plt.grid(True)
    plt.savefig(os.path.join(output_dir, "conflict_plot.png"))
    plt.close()

print("✅ All tracking and conflict data saved to Google Drive at:", output_dir)

import pickle
import os

# Define target path in Google Drive
output_dir = "/content/drive/MyDrive/Colab Notebooks"

# Create output directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)

# Save trajectories
with open(os.path.join(output_dir, "trajectories.pkl"), "wb") as f:
    pickle.dump(dict(trajectories), f)

# Save speeds
with open(os.path.join(output_dir, "speeds.pkl"), "wb") as f:
    pickle.dump(dict(speeds), f)

# Save accelerations
with open(os.path.join(output_dir, "accelerations.pkl"), "wb") as f:
    pickle.dump(dict(accelerations), f)

# Save MTTC data
with open(os.path.join(output_dir, "mttc_data.pkl"), "wb") as f:
    pickle.dump({k: dict(v) for k, v in mttc_data.items()}, f)

# Save DRAC data
with open(os.path.join(output_dir, "drac_data.pkl"), "wb") as f:
    pickle.dump({k: dict(v) for k, v in drac_data.items()}, f)

print("✅ All tracking data saved to Google Drive at:", output_dir)

!pip install pandas openpyxl

import pandas as pd
import os

# Define target path in Google Drive
output_dir = "/content/drive/MyDrive/Colab Notebooks"
output_excel_path = os.path.join(output_dir, "tracking_data.xlsx")

# Create output directory if it doesn't exist
os.makedirs(output_dir, exist_ok=True)

# Prepare DataFrames
df_trajectories = pd.DataFrame.from_dict(trajectories, orient="index").transpose()
df_speeds = pd.DataFrame.from_dict(speeds, orient="index").transpose()
df_accelerations = pd.DataFrame.from_dict(accelerations, orient="index").transpose()

flattened_mttc = []
for k, v in mttc_data.items():
    for frame_num, value in v.items():
        flattened_mttc.append({"object_id": k, "frame": frame_num, "mttc": value})
df_mttc = pd.DataFrame(flattened_mttc)

flattened_drac = []
for k, v in drac_data.items():
    for frame_num, value in v.items():
        flattened_drac.append({"object_id": k, "frame": frame_num, "drac": value})
df_drac = pd.DataFrame(flattened_drac)

# Write to a single Excel file with multiple sheets
with pd.ExcelWriter(output_excel_path, engine="openpyxl") as writer:
    df_trajectories.to_excel(writer, sheet_name="Trajectories", index=False)
    df_speeds.to_excel(writer, sheet_name="Speeds", index=False)
    df_accelerations.to_excel(writer, sheet_name="Accelerations", index=False)
    df_mttc.to_excel(writer, sheet_name="MTTC", index=False)
    df_drac.to_excel(writer, sheet_name="DRAC", index=False)

print("✅ All tracking data saved to one Excel file at:", output_excel_path)

# Plotting speeds for each vehicle
for tracker_id, speed_values in speeds.items():
    plt.plot(speed_values, label=f'Tracker {tracker_id}')

plt.xlabel('Frame')
plt.ylabel('Speed (km/h)')
plt.title('Speed Variation of Detected Vehicles')
plt.legend()
plt.show()

# Plotting speeds for each vehicle tracked for at least 7 seconds
for tracker_id, speed_values in speeds.items():
    if len(speed_values) >= 7 * video_info.fps and all(speed >= 3 for speed in speed_values):  # Check if tracked for at least 8 seconds and all speeds are >= 5 km/h
        plt.plot(speed_values, label=f'Tracker {tracker_id}')

        # Calculate and plot the average line for the current vehicle
        average_line = np.mean([speed_values[-(7 * video_info.fps):]], axis=0)

trend = np.mean(average_line)
plt.plot(average_line, linestyle='--', color='black', alpha=0.9)
plt.xlabel('Frame')
plt.ylabel('Speed (km/h)')
plt.title('Speed Variation of Vehicles Tracked for at least 7 seconds (Speed >= 3 km/h)')
plt.legend(loc='upper left', bbox_to_anchor=(1, 1))
plt.show()

conflict_points = []
conflict_point_coordinates = []
mttc_threshold = 1.5  # seconds
drac_threshold = 2.5  # units

tracker_ids = list(speeds.keys())

for i, tracker_id_1 in enumerate(tracker_ids):
    for j, tracker_id_2 in enumerate(tracker_ids):
        if tracker_id_1 != tracker_id_2:
            speed_list_1 = speeds[tracker_id_1]
            speed_list_2 = speeds[tracker_id_2]
            acceleration_list_1 = accelerations[tracker_id_1]
            acceleration_list_2 = accelerations[tracker_id_2]
            coord_list_1 = coordinates[tracker_id_1]
            coord_list_2 = coordinates[tracker_id_2]
            if speed_list_1 and speed_list_2 and acceleration_list_1 and acceleration_list_2 and coord_list_1 and coord_list_2:
                speed_1 = speed_list_1[-1]
                speed_2 = speed_list_2[-1]
                acceleration_1 = acceleration_list_1[-1]
                acceleration_2 = acceleration_list_2[-1]
                # If you store (x, y) tuples in coordinates:
                y1 = coord_list_1[-1][1] if isinstance(coord_list_1[-1], (tuple, list, np.ndarray)) else coord_list_1[-1]
                y2 = coord_list_2[-1][1] if isinstance(coord_list_2[-1], (tuple, list, np.ndarray)) else coord_list_2[-1]
                relative_distance = abs(y1 - y2)
                relative_speed = speed_2 - speed_1
                relative_acceleration = acceleration_2 - acceleration_1
                discriminant = relative_speed**2 + 2 * relative_acceleration * relative_distance
                if discriminant < 0 or np.isnan(discriminant):
                    mttc = float('inf')
                else:
                    root1 = (-relative_speed + np.sqrt(discriminant)) / relative_acceleration if relative_acceleration != 0 else float('inf')
                    root2 = (-relative_speed - np.sqrt(discriminant)) / relative_acceleration if relative_acceleration != 0 else float('inf')
                    if root1 > 0 and root2 > 0:
                        mttc = min(root1, root2)
                    elif root1 > 0:
                        mttc = root1
                    elif root2 > 0:
                        mttc = root2
                    else:
                        mttc = float('inf')
                drac = (speed_2 ** 2 - speed_1 ** 2) / (2 * relative_distance) if relative_distance > 0 else float('inf')
                mttc_data[tracker_id_1][tracker_id_2].append(mttc)
                drac_data[tracker_id_1][tracker_id_2].append(drac)
                if mttc < mttc_threshold and drac > drac_threshold:
                    conflict_points.append((tracker_id_1, tracker_id_2))
                    conflict_point_coordinates.append((coord_list_1[-1], coord_list_2[-1]))

print("Total Number of Conflict Points:", len(conflict_points))
print("Conflict Points:", conflict_points)
print("Conflict Point Coordinates:", conflict_point_coordinates)

import os
import cv2
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np

# Ensure the directory exists
os.makedirs('/content', exist_ok=True)

# Simulate last_processed_frame as a blank image (or use your actual frame)
last_processed_frame = np.zeros((720, 1280, 3), dtype=np.uint8)
annotated_frame_visualization = last_processed_frame.copy()

# Extract x, y coordinates for scatter plot
x_coordinates, y_coordinates = zip(*conflict_point_coordinates)

# Draw conflict points
plt.figure(figsize=(last_processed_frame.shape[1] / 100, last_processed_frame.shape[0] / 100))
plt.imshow(cv2.cvtColor(annotated_frame_visualization, cv2.COLOR_BGR2RGB))
plt.scatter(x_coordinates, y_coordinates, c='red', marker='o', s=200)
plt.title("Conflict Points Visualization")
plt.show()

# Save the visualization as an image
pil_image = Image.fromarray(cv2.cvtColor(annotated_frame_visualization, cv2.COLOR_BGR2RGB))
pil_image.save('/content/conflict_points.jpg')

import cv2
# Assuming last_processed_frame is the frame where you want to visualize the conflict points
last_processed_frame = frame.copy()

# Copy the frame for visualization
annotated_frame_visualization = last_processed_frame.copy()

# Extract x, y coordinates for scatter plot
x_coordinates, y_coordinates = zip(*[coord[0] for coord in conflict_point_coordinates])

# Mark all conflict points with red circles
plt.figure(figsize=(last_processed_frame.shape[1] / 100, last_processed_frame.shape[0] / 100))
plt.imshow(cv2.cvtColor(annotated_frame_visualization, cv2.COLOR_BGR2RGB))
plt.scatter(x_coordinates, y_coordinates, c='red', marker='o', s=200)
plt.title("Conflict Points Visualization")
plt.show()
plt.imsave('/content/conflict_points.jpg', annotated_frame_visualization)

conflict_points = []
conflict_point_coordinates = []

mttc_threshold = 1.5  # seconds
drac_threshold = 2.5  # m/s^2 or custom units

tracker_ids = list(speeds.keys())

for i, tracker_id_1 in enumerate(tracker_ids):
    for j, tracker_id_2 in enumerate(tracker_ids):
        if tracker_id_1 >= tracker_id_2:
            continue  # avoid duplicate pair checks

        if not (speeds[tracker_id_1] and speeds[tracker_id_2] and
                accelerations[tracker_id_1] and accelerations[tracker_id_2] and
                coordinates[tracker_id_1] and coordinates[tracker_id_2]):
            continue

        speed_1 = speeds[tracker_id_1][-1]
        speed_2 = speeds[tracker_id_2][-1]
        acc_1 = accelerations[tracker_id_1][-1]
        acc_2 = accelerations[tracker_id_2][-1]

        y1 = coordinates[tracker_id_1][-1][1]
        y2 = coordinates[tracker_id_2][-1][1]
        rel_distance = abs(y1 - y2)

        rel_speed = abs(speed_2 - speed_1)
        rel_acc = abs(acc_2 - acc_1)

        if rel_acc == 0:
            mttc = float('inf')
        else:
            discriminant = rel_speed ** 2 + 2 * rel_acc * rel_distance
            if discriminant < 0:
                mttc = float('inf')
            else:
                sqrt_disc = np.sqrt(discriminant)
                root1 = (-rel_speed + sqrt_disc) / rel_acc
                root2 = (-rel_speed - sqrt_disc) / rel_acc
                mttc_candidates = [t for t in (root1, root2) if t > 0]
                mttc = min(mttc_candidates) if mttc_candidates else float('inf')

        drac = (speed_2 ** 2 - speed_1 ** 2) / (2 * rel_distance) if rel_distance > 0 else float('inf')

        if mttc < mttc_threshold and drac > drac_threshold:
            conflict_points.append((tracker_id_1, tracker_id_2))
            mid_x = (coordinates[tracker_id_1][-1][0] + coordinates[tracker_id_2][-1][0]) / 2
            mid_y = (coordinates[tracker_id_1][-1][1] + coordinates[tracker_id_2][-1][1]) / 2
            conflict_point_coordinates.append((mid_x, mid_y))

